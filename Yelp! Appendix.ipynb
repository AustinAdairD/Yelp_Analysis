{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YELP! APPENDEX\n",
    "\n",
    "> Team B1: Austin Adair, Joshua Ferris, Molly Izenson, and Brandon Pugh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Mongodb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[PyMongo Documentation]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the connection to mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('localhost', 27017)\n",
    "\n",
    "db = client.ferrisj2\n",
    "db.authenticate('ferrisj2','bigdata')\n",
    "\n",
    "businesses_collection = db.businesses\n",
    "reviews_collection = db.reviews\n",
    "tips_collection = db.tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b3362b57f7b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbusinesses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"businesses.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression)\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshould_close\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m             obj = self._get_object_parser(\n\u001b[0;32m--> 534\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m             )\n\u001b[1;32m    536\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'frame'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'series'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m--> 871\u001b[0;31m                 loads(json, precise_float=self.precise_float), dtype=None)\n\u001b[0m\u001b[1;32m    872\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m             decoded = {str(k): v for k, v in compat.iteritems(\n",
      "\u001b[0;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "businesses = pd.read_json(\"businesses.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'businesses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-0ee171db3680>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbusinesses_collection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert_many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbusinesses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'records'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'businesses' is not defined"
     ]
    }
   ],
   "source": [
    "businesses_collection.insert_many(businesses.to_dict('records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153499"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "businesses_collection.count_documents({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('5faf5179ff55def13d9fa68a'),\n",
       " 'address': '10913 Bailey Rd',\n",
       " 'business_id': 'f9NumwFMBDn751xgFiRbNA',\n",
       " 'checkins': 38,\n",
       " 'city': 'Cornelius',\n",
       " 'is_open': 1,\n",
       " 'latitude': 35.4627242,\n",
       " 'longitude': -80.8526119,\n",
       " 'name': 'The Range At Lake Norman',\n",
       " 'zip': 28031,\n",
       " 'review_count': 36,\n",
       " 'stars': 3.5,\n",
       " 'state': 'NC',\n",
       " 'zbp_employees': 8164,\n",
       " 'zbp_establishments': 907,\n",
       " 'zbp_annual_payroll': 284254}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "businesses_collection.find_one({\"business_id\": \"f9NumwFMBDn751xgFiRbNA\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x7fb4f5e4f5c8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = pd.read_csv(\"reviews.csv\")\n",
    "reviews_collection.insert_many(reviews.to_dict('records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8021124"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_collection.count_documents({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('5faf52fdff55def13da1fe2a'),\n",
       " 'business_id': 'IS4cv902ykd8wj1TR0N3-A',\n",
       " 'cool': 0,\n",
       " 'date': '2017-01-14 21:56:57',\n",
       " 'funny': 0,\n",
       " 'review_id': '6TdNDKywdbjoTkizeMce8A',\n",
       " 'stars': 4.0,\n",
       " 'text': 'Oh happy day, finally have a Canes near my casa. Yes just as others are griping about the Drive thru is packed just like most of the other canes in the area but I like to go sit down to enjoy my chicken. The cashiers are pleasant and as far as food wise i have yet to receive any funky chicken. The clean up crew zips around the dining area constantly so it\\'s usually well kept. My only gripe is the one fella with Red hair he makes the rounds while cleaning but no smile or personality a few nights ago he tossed the napkins i just put on the table to help go with my meal. After I was done he just reached for my tray no \"excuse me or are you done with that?\"  I realize he\\'s trying to do his job quickly but a little table manners goes along way. That being said still like to grub here and glad that there\\'s finally a Cane\\'s close to me.',\n",
       " 'useful': 0.0,\n",
       " 'user_id': 'UgMW8bLE0QMJDCkQ1Ax5Mg',\n",
       " 'Date': '2017-01-14',\n",
       " 'Time': '21:56:57'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_collection.find_one({\"review_id\": \"6TdNDKywdbjoTkizeMce8A\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Dataset Exploration\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import urllib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209393"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "businesses = []\n",
    "for line in open('yelp_academic_dataset_business.json', 'r'):\n",
    "    businesses.append(json.loads(line))\n",
    "len(businesses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175187"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkins = []\n",
    "for line in open('yelp_academic_dataset_checkin.json', 'r'):\n",
    "    checkins.append(json.loads(line))\n",
    "len(checkins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Checkins to businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkins_dict = {}\n",
    "for checkin in checkins:\n",
    "    cins = checkin[\"date\"].split(',')\n",
    "    checkins_dict[checkin[\"business_id\"]] = len(cins)\n",
    "for i in range(0, len(businesses)):\n",
    "    businesses[i][\"checkins\"] = checkins_dict.get(businesses[i][\"business_id\"], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix Columns and Reduce to businesses in the US\n",
    "\n",
    "> Specificially AZ, IL, NC, NV, OH, PA, SC, WI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153499, 12)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = json_normalize(businesses)\n",
    "df[\"categories\"] = df[\"categories\"].fillna(\"Unknown\")\n",
    "df.drop(list(df.filter(regex = 'attributes|hours')), axis=\"columns\", inplace=True)\n",
    "df.drop([\"categories\"], axis=\"columns\", inplace=True)\n",
    "df.rename(columns = {'postal_code':'zip'}, inplace = True)\n",
    "df = df[df.zip.str.isdigit()]\n",
    "acceptable_states = [\"AZ\", \"IL\", \"NC\", \"NV\", \"OH\", \"PA\", \"SC\", \"WI\"]\n",
    "df = df[df.state.isin(acceptable_states)]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch 2018 ZBP Census data by zip code\n",
    "\n",
    "> ZIP Code Business Patterns (ZBP) is an annual series that provides economic data by ZIP Code. This table includes the number of establishments, employment during the week of March 12, first quarter payroll, and annual payroll for All Industries (NAICS 00) by 5-digit ZIP Code.\n",
    "\n",
    "https://api.census.gov/data/2018/zbp.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "zips = list(set(df[\"zip\"]))\n",
    "baseurl = \"https://api.census.gov/data/2010/zbp?\"\n",
    "q = {\"get\":'EMP,ESTAB,PAYANN', \"for\":\"zipcode:\"+','.join(zips),}\n",
    "fullurl = baseurl + urllib.parse.urlencode(q)\n",
    "result = urllib.request.urlopen(fullurl).read()\n",
    "resd = json.loads(result)\n",
    "with open('zbp_data.json', 'w') as outfile:\n",
    "    json.dump(resd, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Census Data to Businesses Dataframe\n",
    "\n",
    "> First impute missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zbp_data = pd.DataFrame(resd[1:], columns=[\"zbp_employees\", \"zbp_establishments\", \"zbp_annual_payroll\", \"zip\"])\n",
    "zbp_data[\"zbp_employees\"] = pd.to_numeric(zbp_data['zbp_employees'])\n",
    "zbp_data[\"zbp_establishments\"] = pd.to_numeric(zbp_data['zbp_establishments'])\n",
    "zbp_data[\"zbp_annual_payroll\"] = pd.to_numeric(zbp_data['zbp_annual_payroll'])\n",
    "zbp_data.replace(0, np.nan, inplace=True)\n",
    "zbp_employees_median = zbp_data[\"zbp_employees\"].median()\n",
    "zbp_establishments_median = zbp_data[\"zbp_establishments\"].median()\n",
    "zbp_annual_payroll_median = zbp_data[\"zbp_annual_payroll\"].median()\n",
    "zbp_data.fillna(zbp_data.median(),inplace=True)\n",
    "\n",
    "new_data = {\n",
    "    \"zbp_employees\": [],\n",
    "    \"zbp_establishments\": [],\n",
    "    \"zbp_annual_payroll\": [],\n",
    "}\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    cur_zbp = zbp_data[zbp_data[\"zip\"] == row[\"zip\"]]\n",
    "    if cur_zbp.empty:\n",
    "        new_data[\"zbp_employees\"].append(zbp_employees_median)\n",
    "        new_data[\"zbp_establishments\"].append(zbp_establishments_median)\n",
    "        new_data[\"zbp_annual_payroll\"].append(zbp_annual_payroll_median)\n",
    "    else:\n",
    "        new_data[\"zbp_employees\"].append(cur_zbp.iloc[0][\"zbp_employees\"])\n",
    "        new_data[\"zbp_establishments\"].append(cur_zbp.iloc[0][\"zbp_establishments\"])\n",
    "        new_data[\"zbp_annual_payroll\"].append(cur_zbp.iloc[0][\"zbp_annual_payroll\"])\n",
    "\n",
    "df[\"zbp_employees\"] = new_data[\"zbp_employees\"]\n",
    "df[\"zbp_establishments\"] = new_data[\"zbp_establishments\"]\n",
    "df[\"zbp_annual_payroll\"] = new_data[\"zbp_annual_payroll\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe(include = 'object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_open'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby(\"state\")[\"checkins\", \"stars\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(column=\"stars\",bins=5,grid=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"state\").count()['business_id'].plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['state','is_open']).count()['business_id'].unstack().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x=\"stars\", y=\"checkins\", kind=\"scatter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['state','is_open']).mean()['stars'].unstack().plot(kind=\"line\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['state','is_open']).mean()['checkins'].unstack().plot(kind=\"line\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(r'businesses.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_data = []\n",
    "\n",
    "file = open('yelp_academic_dataset_review.json')\n",
    "for line in file:\n",
    "    json_line = json.loads(line)\n",
    "    json_data.append(json_line)\n",
    "\n",
    "print(json_data [3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "json_data = pd.DataFrame(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((json_data == 0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((json_data['text']==\"\").sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data['Date'] = pd.to_datetime(json_data['date']).dt.date\n",
    "json_data['Time'] = pd.to_datetime(json_data['date']).dt.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data['cool'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data.drop(['useful', 'funny', 'cool'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data['business_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data.to_csv ('reviews.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "This file will iterate over all businesses located in Ohio and run sentiment analysis over the associated reviews for each business using a partially clean and fully clean corpus.\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, string, nltk\n",
    "from pymongo import MongoClient\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('localhost', 27017)\n",
    "db = client.ferrisj2\n",
    "db.authenticate('ferrisj2','bigdata')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_sentiment(business_id, review_total):\n",
    "    ss_neg_arr = []\n",
    "    ss_neu_arr = []\n",
    "    ss_pos_arr = []\n",
    "    ss_compound_arr = []\n",
    "    for review in db.reviews.find({'business_id': business_id}):\n",
    "        current_text = review['text']\n",
    "        \n",
    "        tokens = word_tokenize(current_text)\n",
    "\n",
    "        # Remove the punctuations and numbers\n",
    "        tokens = [word for word in tokens if word.isalpha()]\n",
    "\n",
    "        # Lower the tokens\n",
    "        tokens = [word.lower() for word in tokens]\n",
    "\n",
    "        # Remove stopword\n",
    "        tokens = [word for word in tokens if not word in stopwords.words(\"english\")]\n",
    "\n",
    "        # Stem the tokens\n",
    "        ps = PorterStemmer()\n",
    "        tokens = [ps.stem(word) for word in tokens]\n",
    "\n",
    "        text_clean = \" \".join(tokens)\n",
    "        \n",
    "        sia = SentimentIntensityAnalyzer()\n",
    "        ss = sia.polarity_scores(text_clean)\n",
    "        \n",
    "        ss_neg_arr.append(ss['neg'])\n",
    "        ss_neu_arr.append(ss['neu'])\n",
    "        ss_pos_arr.append(ss['pos'])\n",
    "        ss_compound_arr.append(ss['compound'])\n",
    "\n",
    "        db.reviews.update_one({\n",
    "            'review_id': review['review_id']\n",
    "        },\n",
    "        {\n",
    "            '$set': {\n",
    "                'text_clean':text_clean,\n",
    "                'ss_neg': ss['neg'],\n",
    "                'ss_neu': ss['neu'],\n",
    "                'ss_pos': ss['pos'],\n",
    "                'ss_compound': ss['compound']\n",
    "            }\n",
    "        })\n",
    "    return sum(ss_neg_arr) / review_total, sum(ss_neu_arr) / review_total, sum(ss_pos_arr) / review_total, sum(ss_compound_arr) / review_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "business_cursor = db.businesses.find({'state': 'WI', 'ss_compound': {'$exists': False}}, batch_size=4, no_cursor_timeout=True)\n",
    "\n",
    "for business in business_cursor:\n",
    "    print(datetime.datetime.now().strftime('%d/%m/%Y %H:%M:%S'), ' | Business: ', business['business_id'])\n",
    "    review_total = db.reviews.find({ 'business_id': business['business_id']}).count()\n",
    "    print('# of reviews: ', str(review_total))\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    if review_total > 0 :\n",
    "        ss_neg, ss_neu, ss_pos, ss_compound = review_sentiment(business['business_id'], review_total)\n",
    "        db.businesses.update_one({'business_id': business['business_id']}, {\n",
    "            '$set': {'ss_neg': ss_neg, 'ss_neu': ss_neu, 'ss_pos': ss_pos, 'ss_compound': ss_compound}\n",
    "        })\n",
    "    else:\n",
    "        db.businesses.update_one({'business_id': business['business_id']}, {\n",
    "            '$set': {'ss_neg': 0, 'ss_neu': 0, 'ss_pos': 0, 'ss_compound': 0}\n",
    "        })\n",
    "\n",
    "    end_time = datetime.datetime.now()\n",
    "    print('Procssing time: ', str(end_time-start_time))\n",
    "    print('-'*80)\n",
    "\n",
    "business_cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Todo: \", db.businesses.find({'state': 'WI', 'ss_compound': {'$exists': False}}).count())\n",
    "print(\"Done: \", db.businesses.find({'state': 'WI', 'ss_compound': {'$exists': True}}).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "businesses_arr = list(db.businesses.find({'state': 'WI', 'ss_compound': {'$exists': True}}))\n",
    "stars = [d['stars'] for d in businesses_arr]\n",
    "checkins = [d['checkins'] for d in businesses_arr]\n",
    "\n",
    "minStars = min(stars)\n",
    "maxStars = max(stars)\n",
    "minCheckins = min(checkins)\n",
    "maxCheckins = max(checkins)\n",
    "\n",
    "for business in businesses_arr:\n",
    "    print(datetime.datetime.now().strftime('%d/%m/%Y %H:%M:%S'), ' | Business: ', business['business_id'])\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    stars_normalized = (business['stars']-minStars)/(maxStars-minStars)\n",
    "    checkins_normalized = (business['checkins']-minCheckins)/(maxCheckins-minCheckins)\n",
    "    target = abs(stars_normalized - checkins_normalized)\n",
    "    \n",
    "    print(target)\n",
    "    \n",
    "    db.businesses.update_one({'business_id': business['business_id']}, {\n",
    "        '$set': {'target': target}\n",
    "    })\n",
    "    \n",
    "    end_time = datetime.datetime.now()\n",
    "    print('Procssing time: ', str(end_time-start_time))\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.businesses.find_one({'target': {'$exists': True}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy import arange\n",
    "from pymongo import MongoClient\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor, VotingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import explained_variance_score, max_error, mean_absolute_error, mean_squared_error, mean_squared_error, mean_squared_log_error, median_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('localhost', 27017)\n",
    "db = client.ferrisj2\n",
    "db.authenticate('ferrisj2','bigdata')\n",
    "\n",
    "b_arr = list(db.businesses.find({'state': 'WI', 'target': {'$exists': True}}))\n",
    "\n",
    "df = pd.DataFrame(b_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['review_count','zbp_employees','zbp_establishments','zbp_annual_payroll','ss_compound','ss_neg','ss_neu','ss_pos']]\n",
    "y = df[['target']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Linear Regression\n",
    "### Original Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train.values.ravel())\n",
    "y_pred = regressor.predict(X_test)\n",
    "print('MSE: ', mean_squared_error(y_test, y_pred), ' | R2: ', r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.api import OLS\n",
    "OLS(y_train,X_train).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lasso Model\n",
    "### Original Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = Lasso()\n",
    "regressor.fit(X_train, y_train.values.ravel())\n",
    "y_pred = regressor.predict(X_test)\n",
    "print('MSE: ', mean_squared_error(y_test, y_pred), ' | R2: ', r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunegrid = {\n",
    "    'alpha': arange(0.01, 0.12, 0.01)\n",
    "}\n",
    "\n",
    "regressor = Lasso()\n",
    "grid_search = GridSearchCV(estimator=regressor, param_grid=tunegrid, cv=10, n_jobs=-1, scoring='r2')\n",
    "grid_search.fit(X_train, y_train.values.ravel())\n",
    "best_grid = grid_search.best_estimator_\n",
    "y_pred = best_grid.predict(X_test)\n",
    "print(best_grid)\n",
    "print('MSE: ', mean_squared_error(y_test, y_pred), ' | R2: ', r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Elastic Model\n",
    "### Original Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = ElasticNet(random_state=1337)\n",
    "regressor.fit(X_train, y_train.values.ravel())\n",
    "y_pred = regressor.predict(X_test)\n",
    "print('MSE: ', mean_squared_error(y_test, y_pred), ' | R2: ', r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunegrid = {\n",
    "    'alpha': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.0, 1.0, 10.0, 100.0],\n",
    "    'l1_ratio': arange(0, 1, 0.01),\n",
    "    'max_iter': range(1000, 6000, 1000)\n",
    "}\n",
    "\n",
    "regressor = ElasticNet(random_state=1337)\n",
    "grid_search = GridSearchCV(estimator=regressor, param_grid=tunegrid, cv=10, n_jobs=-1, scoring='r2')\n",
    "grid_search.fit(X_train, y_train.values.ravel())\n",
    "best_grid = grid_search.best_estimator_\n",
    "y_pred = best_grid.predict(X_test)\n",
    "print(best_grid)\n",
    "print('MSE: ', mean_squared_error(y_test, y_pred), ' | R2: ', r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ADABoost\n",
    "### Original Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = AdaBoostRegressor(random_state=1337)\n",
    "regressor.fit(X_train, y_train.values.ravel())\n",
    "y_pred = regressor.predict(X_test)\n",
    "print('MSE: ', mean_squared_error(y_test, y_pred), ' | R2: ', r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunegrid = {\n",
    "    'n_estimators': range(10, 50, 5),\n",
    "    'learning_rate': arange(0.01, 0.2, 0.01),\n",
    "    'loss': ['linear', 'square', 'exponential'],\n",
    "}\n",
    "\n",
    "regressor = AdaBoostRegressor(random_state=1337)\n",
    "grid_search = GridSearchCV(estimator=regressor, param_grid=tunegrid, cv=10, n_jobs=-1, scoring='r2')\n",
    "grid_search.fit(X_train, y_train.values.ravel())\n",
    "best_grid = grid_search.best_estimator_\n",
    "y_pred = best_grid.predict(X_test)\n",
    "print(best_grid)\n",
    "print('MSE: ', mean_squared_error(y_test, y_pred), ' | R2: ', r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Gradient Boosting\n",
    "### Original Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = GradientBoostingRegressor(random_state=1337)\n",
    "regressor.fit(X_train, y_train.values.ravel())\n",
    "y_pred = regressor.predict(X_test)\n",
    "print('MSE: ', mean_squared_error(y_test, y_pred), ' | R2: ', r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunegrid = {\n",
    "    'n_estimators': range(100, 500, 100),\n",
    "    'learning_rate': range(0.02, 0.12, 0.02),\n",
    "    'max_depth': range(1, 5),\n",
    "    'loss': ['huber'],\n",
    "    'alpha': arange(0.1, 0.3, 0.1),\n",
    "}\n",
    "\n",
    "regressor = GradientBoostingRegressor(random_state=1337)\n",
    "grid_search = GridSearchCV(estimator=regressor, param_grid=tunegrid, cv=10, n_jobs=-1, scoring='r2')\n",
    "grid_search.fit(X_train, y_train.values.ravel())\n",
    "best_grid = grid_search.best_estimator_\n",
    "y_pred = best_grid.predict(X_test)\n",
    "print(best_grid)\n",
    "print('MSE: ', mean_squared_error(y_test, y_pred), ' | R2: ', r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Decision Tree\n",
    "### Default Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = DecisionTreeRegressor(random_state=1337, max_depth=10, min_samples_split=5, max_leaf_nodes=12)\n",
    "regressor.fit(X_train, y_train.values.ravel())\n",
    "y_pred = regressor.predict(X_test)\n",
    "print('MSE: ', mean_squared_error(y_test, y_pred), ' | R2: ', r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth': range(3, 11),\n",
    "    'min_samples_split': range(20, 120, 20),\n",
    "    'max_leaf_nodes': range(4, 20, 2)\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=regressor, param_grid=param_grid, cv=10, n_jobs=-1, scoring='r2')\n",
    "grid_search.fit(X_train, y_train.values.ravel())\n",
    "best_grid = grid_search.best_estimator_\n",
    "y_pred = best_grid.predict(X_test)\n",
    "print(best_grid)\n",
    "print('MSE: ', mean_squared_error(y_test, y_pred), ' | R2: ', r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Random Forest\n",
    "### Default Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = RandomForestRegressor(random_state = 1337)\n",
    "regressor.fit(X_train, y_train.values.ravel())\n",
    "y_pred = regressor.predict(X_test)\n",
    "print('MSE: ', mean_squared_error(y_test, y_pred), ' | R2: ', r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': range(3, 11),\n",
    "    'max_features': range(1,8),\n",
    "    'min_samples_leaf': range(3,10),\n",
    "    'min_samples_split': range(20, 120, 20),\n",
    "    'n_estimators': range(300, 700, 100)\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator = regressor, param_grid = param_grid, cv = 10, n_jobs = -1, scoring='r2')\n",
    "grid_search.fit(X_train, y_train.values.ravel())\n",
    "best_grid = grid_search.best_estimator_\n",
    "y_pred = best_grid.predict(X_test)\n",
    "print(best_grid)\n",
    "print('MSE: ', mean_squared_error(y_test, y_pred), ' | R2: ', r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
